---
title: "CULG: Commercial Universal Language Generation"
collection: publications
permalink: /publication/2022-naacl-culg
year: 2022
date: 2022-07-10
venue: 'Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Industry Track'
paperurl: ''
author: '<b>Haonan Li</b>, Yameng Huang, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, Nan Duan'
---

```

```

## Abstract
Pre-trained language models (PLMs) have dramatically improved performance for many natural language processing (NLP) tasks in domains such as finance and healthcare.
However, the application of PLMs in the domain of commerce, especially marketing and advertising, remains less studied.
In this work, we adapt pre-training methods to the domain of commerce, by proposing CULG, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across 7 languages.
We propose 4 commercial generation tasks and a two-stage training strategy for pre-training, and demonstrate that the proposed strategy yields performance improvements on three generation tasks as compared to single-stage pre-training.
Extensive experiments show that our model outperforms other models by a large margin on commercial generation tasks.
