---
title: "CULG: Commercial Universal Language Generation"
collection: publications
permalink: /publication/2022-naacl-culg
year: 2022
date: 2022-07-10
venue: 'Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Industry Track'
paperurl: 'https://aclanthology.org/2022.naacl-industry.14/'
author: '<b>Haonan Li</b>, Yameng Huang, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, Nan Duan'
---

```
@inproceedings{li-etal-2022-culg,
    title = "{CULG}: Commercial Universal Language Generation",
    author = "Li, Haonan  and
      Huang, Yameng  and
      Gong, Yeyun  and
      Jiao, Jian  and
      Zhang, Ruofei  and
      Baldwin, Timothy  and
      Duan, Nan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track",
    month = jul,
    year = "2022",
    address = "Hybrid: Seattle, Washington + Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-industry.14",
    pages = "112--120",
    abstract = "Pre-trained language models (PLMs) have dramatically improved performance for many natural language processing (NLP) tasks in domains such as finance and healthcare. However, the application of PLMs in the domain of commerce, especially marketing and advertising, remains less studied. In this work, we adapt pre-training methods to the domain of commerce, by proposing CULG, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across 7 languages. We propose 4 commercial generation tasks and a two-stage training strategy for pre-training, and demonstrate that the proposed strategy yields performance improvements on three generation tasks as compared to single-stage pre-training. Extensive experiments show that our model outperforms other models by a large margin on commercial generation tasks, and we conclude with a discussion on additional applications over other markets, languages, and tasks.",
}
```

## Abstract
Pre-trained language models (PLMs) have dramatically improved performance for many natural language processing (NLP) tasks in domains such as finance and healthcare.
However, the application of PLMs in the domain of commerce, especially marketing and advertising, remains less studied.
In this work, we adapt pre-training methods to the domain of commerce, by proposing CULG, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across 7 languages.
We propose 4 commercial generation tasks and a two-stage training strategy for pre-training, and demonstrate that the proposed strategy yields performance improvements on three generation tasks as compared to single-stage pre-training.
Extensive experiments show that our model outperforms other models by a large margin on commercial generation tasks.
