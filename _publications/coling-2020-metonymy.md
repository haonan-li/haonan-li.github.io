---
title: "Target Word Masking for Location Metonymy Resolution"
collection: publications
permalink: /publication/coling-2020-metonymy
year: 2020
date: 2020-11-08
venue: 'Proceedings of the 28th International Conference on Computational Linguistics (COLING)'
paperurl: 'https://aclanthology.org/2020.coling-main.330/'
author: '<b>Haonan Li</b>, Maria Vasardani, Martin Tomko, Timothy Baldwin'
---

```
@inproceedings{li-etal-2020-target,
    title = "Target Word Masking for Location Metonymy Resolution",
    author = "Li, Haonan  and
      Vasardani, Maria  and
      Tomko, Martin  and
      Baldwin, Timothy",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.330",
    doi = "10.18653/v1/2020.coling-main.330",
    pages = "3696--3707",
    abstract = "Existing metonymy resolution approaches rely on features extracted from external resources like dictionaries and hand-crafted lexical resources. In this paper, we propose an end-to-end word-level classification approach based only on BERT, without dependencies on taggers, parsers, curated dictionaries of place names, or other external resources. We show that our approach achieves the state-of-the-art on 5 datasets, surpassing conventional BERT models and benchmarks by a large margin. We also show that our approach generalises well to unseen data.",
}
```

## Abstract
Existing metonymy resolution approaches rely on features extracted from external resources like dictionaries and hand-crafted lexical resources. In this paper, we propose an end-to-end word-level classification approach based only on BERT, without dependencies on taggers, parsers, curated dictionaries of place names, or other external resources. We show that our approach achieves the state-of-the-art on 5 datasets, surpassing conventional BERT models and benchmarks by a large margin. We also show that our approach generalises well to unseen data.
